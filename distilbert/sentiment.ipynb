{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use language models to predict the sentiment of a given movie review. The dataset is sampled from the [IMDB dataset of 50k movie reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews). The sentences are sampled to a smaller set to help with quicker computation on Colab. The data contains a review and an associated _positive_ or _negative_ sentiment label. The training, validation, and test data is used to fine-tune the models, select the best model while training, and measure performance, respectively. To perform this task, we use a pre-trained DistilBERT model, a BERT-based language model that is 40% smaller, 60% faster, but retains 97% of BERT's performance, from Hugging Face, as follows:\n",
    "\n",
    "1. Load a pre-trained DistilBERT model and its tokenizer using Hugging Face's `AutoTokenizer` and `AutoModelForSequenceClassification` classes, adapting the model to the task of sentiment analysis.\n",
    "2. Tokenize the reviews and convert the labels into numerical classes by using the `tokenizer.encode_plus` method, which takes in a review and returns a dictionary that contains the tokenized review as a Tensor; convert labels to a numerical class using the label_dict dictionary.\n",
    "3. Set the layers of the model to be trained based on the `training_type` by iterating over the named parameters of the model and setting `requires_grad=False` for the layers that are not to be trained. The classifier head on top of the final DistilBERT layer is always trained, so this layer is always set to be trainable.\n",
    "4. Train the model, updating parameters by backpropagating the loss.\n",
    "5. Validate and test the model by computing the Precision, Recall, and F1.\n",
    "\n",
    "The training types are:\n",
    "\n",
    "- frozen_embeddings: All layers are frozen.\n",
    "- top_2_training: Only the last two layers are trained.\n",
    "- top_4_training: Only the last four layers are trained.\n",
    "- all_training: All layers are trained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    %cd \"drive/MyDrive/M4L/distilbert\"\n",
    "except:\n",
    "    pass\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "SAVE_PATH = \"models/DistilBERT\"\n",
    "\n",
    "train_data = pd.read_csv(\"data/train_data.csv\")\n",
    "val_data = pd.read_csv(\"data/val_data.csv\")\n",
    "test_data = pd.read_csv(\"data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillBERT:\n",
    "    def __init__(\n",
    "        self, model_name=\"distilbert-base-uncased\", num_classes=2\n",
    "    ):  # num_classes = 2 for binary classification\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=num_classes\n",
    "        )\n",
    "\n",
    "    def get_tokenizer_and_model(self):\n",
    "        return self.model, self.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def tokenize_data(self):\n",
    "        print(\"Processing data..\")\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        label_dict = {\"positive\": 1, \"negative\": 0}\n",
    "\n",
    "        review_list = self.data[\"review\"].to_list()\n",
    "        label_list = self.data[\"sentiment\"].to_list()\n",
    "\n",
    "        for review, label in tqdm(zip(review_list, label_list), total=len(review_list)):\n",
    "            # tokenize the review\n",
    "            encoded_review = self.tokenizer.encode_plus(\n",
    "                review,\n",
    "                max_length=512,  # limit the max token length to 512\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            tokens.append(encoded_review[\"input_ids\"].squeeze())\n",
    "            # convert the labels to the corresponding numerical classes\n",
    "            labels.append(label_dict[label])\n",
    "\n",
    "        tokens = pad_sequence(tokens, batch_first=True)\n",
    "        labels = torch.tensor(labels).to(\n",
    "            \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        dataset = TensorDataset(tokens, labels)\n",
    "        return dataset\n",
    "\n",
    "    def get_data_loaders(self, batch_size=32, shuffle=True):\n",
    "        processed_dataset = self.tokenize_data()\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            processed_dataset, shuffle=shuffle, batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, options):\n",
    "        self.device = options[\"device\"]\n",
    "        self.train_data = options[\"train_data\"]\n",
    "        self.val_data = options[\"val_data\"]\n",
    "        self.batch_size = options[\"batch_size\"]\n",
    "        self.epochs = options[\"epochs\"]\n",
    "        self.save_path = options[\"save_path\"]\n",
    "        self.training_type = options[\"training_type\"]\n",
    "        transformer = DistillBERT()\n",
    "        self.model, self.tokenizer = transformer.get_tokenizer_and_model()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def get_performance_metrics(self, preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        precision = precision_score(labels_flat, pred_flat, zero_division=0)\n",
    "        recall = recall_score(labels_flat, pred_flat, zero_division=0)\n",
    "        f1 = f1_score(labels_flat, pred_flat, zero_division=0)\n",
    "        return precision, recall, f1\n",
    "\n",
    "    def set_training_parameters(self):\n",
    "        def set_requires_grad(layer_threshold=None):\n",
    "            for name, layer in self.model.named_parameters():\n",
    "                # freeze layers as per the layer_threshold\n",
    "                if \"classifier\" not in name:\n",
    "                    if layer_threshold is not None:\n",
    "                        layer_number = name.split(\".\")[2]\n",
    "                        if layer_number.isdigit():\n",
    "                            layer.requires_grad = int(layer_number) >= layer_threshold\n",
    "                        else:\n",
    "                            layer.requires_grad = False\n",
    "                    else:\n",
    "                        layer.requires_grad = False\n",
    "\n",
    "        # set the layers to be trained based on the training_type\n",
    "        if self.training_type == \"frozen_embeddings\":\n",
    "            set_requires_grad()\n",
    "        elif self.training_type == \"top_2_training\":\n",
    "            set_requires_grad(4)\n",
    "        elif self.training_type == \"top_4_training\":\n",
    "            set_requires_grad(2)\n",
    "        elif self.training_type == \"all_training\":\n",
    "            for layer in self.model.parameters():\n",
    "                layer.requires_grad = True\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid training_type: {self.training_type}\")\n",
    "\n",
    "    def train(self, data_loader, optimizer):\n",
    "        self.model.train()\n",
    "        total_recall = 0\n",
    "        total_precision = 0\n",
    "        total_f1 = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, (reviews, labels) in enumerate(tqdm(data_loader)):\n",
    "            self.model.zero_grad()\n",
    "            reviews = reviews.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            # get outputs from the model\n",
    "            outputs = self.model(reviews, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # backpropagate the loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate metrics\n",
    "            logits = outputs.logits.detach().cpu().numpy()\n",
    "            label_ids = labels.to(\"cpu\").numpy()\n",
    "            precision, recall, f1 = self.get_performance_metrics(logits, label_ids)\n",
    "\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f1\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        precision = total_precision / len(data_loader)\n",
    "        recall = total_recall / len(data_loader)\n",
    "        f1 = total_f1 / len(data_loader)\n",
    "        loss = total_loss / len(data_loader)\n",
    "\n",
    "        return precision, recall, f1, loss\n",
    "\n",
    "    def eval(self, data_loader):\n",
    "        self.model.eval()\n",
    "        total_recall = 0\n",
    "        total_precision = 0\n",
    "        total_f1 = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for reviews, labels in tqdm(data_loader):\n",
    "                reviews = reviews.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                # get outputs from the model\n",
    "                outputs = self.model(reviews, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                # don't backpropagate the loss\n",
    "                logits = outputs.logits.detach().cpu().numpy()\n",
    "                label_ids = labels.to(\"cpu\").numpy()\n",
    "\n",
    "                # calculate metrics\n",
    "                precision, recall, f1 = self.get_performance_metrics(logits, label_ids)\n",
    "\n",
    "                total_precision += precision\n",
    "                total_recall += recall\n",
    "                total_f1 += f1\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        precision = total_precision / len(data_loader)\n",
    "        recall = total_recall / len(data_loader)\n",
    "        f1 = total_f1 / len(data_loader)\n",
    "        loss = total_loss / len(data_loader)\n",
    "\n",
    "        return precision, recall, f1, loss\n",
    "\n",
    "    def save_transformer(self):\n",
    "        self.model.save_pretrained(self.save_path)\n",
    "        self.tokenizer.save_pretrained(self.save_path)\n",
    "\n",
    "    def execute(self):\n",
    "        last_best = 0\n",
    "        train_dataset = DatasetLoader(self.train_data, self.tokenizer)\n",
    "        train_data_loader = train_dataset.get_data_loaders(self.batch_size)\n",
    "        val_dataset = DatasetLoader(self.val_data, self.tokenizer)\n",
    "        val_data_loader = val_dataset.get_data_loaders(self.batch_size)\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=3e-5, eps=1e-8)\n",
    "        self.set_training_parameters()\n",
    "        for epoch_i in range(0, self.epochs):\n",
    "            train_precision, train_recall, train_f1, train_loss = self.train(\n",
    "                train_data_loader, optimizer\n",
    "            )\n",
    "            print(\n",
    "                f\"Epoch {epoch_i + 1}: train_loss: {train_loss:.4f} train_precision: {train_precision:.4f} train_recall: {train_recall:.4f} train_f1: {train_f1:.4f}\"\n",
    "            )\n",
    "            val_precision, val_recall, val_f1, val_loss = self.eval(val_data_loader)\n",
    "            print(\n",
    "                f\"Epoch {epoch_i + 1}: val_loss: {val_loss:.4f} val_precision: {val_precision:.4f} val_recall: {val_recall:.4f} val_f1: {val_f1:.4f}\"\n",
    "            )\n",
    "\n",
    "            if val_f1 > last_best:\n",
    "                print(\"Saving model..\")\n",
    "                self.save_transformer()\n",
    "                last_best = val_f1\n",
    "                print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5130/5130 [00:14<00:00, 363.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:01<00:00, 167.94it/s]\n",
      "100%|██████████| 321/321 [01:25<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.6780 train_precision: 0.5945 train_recall: 0.6750 train_f1: 0.5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: val_loss: 0.6616 val_precision: 0.7134 val_recall: 0.4446 val_f1: 0.5381\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:29<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.6436 train_precision: 0.7010 train_recall: 0.6986 train_f1: 0.6741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: val_loss: 0.6240 val_precision: 0.7708 val_recall: 0.6055 val_f1: 0.6690\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:29<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.6024 train_precision: 0.7286 train_recall: 0.7228 train_f1: 0.7023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: val_loss: 0.5843 val_precision: 0.7337 val_recall: 0.7996 val_f1: 0.7524\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "options = {}\n",
    "options[\"batch_size\"] = BATCH_SIZE\n",
    "options[\"device\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "options[\"train_data\"] = train_data\n",
    "options[\"val_data\"] = val_data\n",
    "options[\"save_path\"] = SAVE_PATH + \"_frozen_embeddings\"\n",
    "options[\"epochs\"] = EPOCHS\n",
    "options[\"training_type\"] = \"frozen_embeddings\"\n",
    "trainer = Trainer(options)\n",
    "trainer.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 2 Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5130/5130 [00:09<00:00, 546.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 785.98it/s]\n",
      "100%|██████████| 321/321 [01:30<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.6793 train_precision: 0.6029 train_recall: 0.6540 train_f1: 0.5854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: val_loss: 0.6582 val_precision: 0.6471 val_recall: 0.7163 val_f1: 0.6726\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:29<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.6397 train_precision: 0.7000 train_recall: 0.7267 train_f1: 0.6861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: val_loss: 0.6200 val_precision: 0.7257 val_recall: 0.7208 val_f1: 0.7083\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:29<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.6014 train_precision: 0.7329 train_recall: 0.7329 train_f1: 0.7116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: val_loss: 0.5842 val_precision: 0.7117 val_recall: 0.8149 val_f1: 0.7556\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "options = {}\n",
    "options[\"batch_size\"] = BATCH_SIZE\n",
    "options[\"device\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "options[\"train_data\"] = train_data\n",
    "options[\"val_data\"] = val_data\n",
    "options[\"save_path\"] = SAVE_PATH + \"_top_2_training\"\n",
    "options[\"epochs\"] = EPOCHS\n",
    "options[\"training_type\"] = \"top_2_training\"\n",
    "trainer = Trainer(options)\n",
    "trainer.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 4 Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5130/5130 [00:09<00:00, 549.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 802.19it/s]\n",
      "100%|██████████| 321/321 [01:30<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.6788 train_precision: 0.6080 train_recall: 0.6644 train_f1: 0.5997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: val_loss: 0.6587 val_precision: 0.6181 val_recall: 0.9123 val_f1: 0.7295\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:30<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.6422 train_precision: 0.6790 train_recall: 0.7100 train_f1: 0.6575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: val_loss: 0.6210 val_precision: 0.7169 val_recall: 0.8036 val_f1: 0.7498\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:30<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.6043 train_precision: 0.7282 train_recall: 0.7324 train_f1: 0.7083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: val_loss: 0.5873 val_precision: 0.7543 val_recall: 0.6898 val_f1: 0.7105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "options = {}\n",
    "options[\"batch_size\"] = BATCH_SIZE\n",
    "options[\"device\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "options[\"train_data\"] = train_data\n",
    "options[\"val_data\"] = val_data\n",
    "options[\"save_path\"] = SAVE_PATH + \"_top_4_training\"\n",
    "options[\"epochs\"] = EPOCHS\n",
    "options[\"training_type\"] = \"top_4_training\"\n",
    "trainer = Trainer(options)\n",
    "trainer.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5130/5130 [00:06<00:00, 780.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 758.50it/s]\n",
      "100%|██████████| 321/321 [04:09<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.3464 train_precision: 0.8441 train_recall: 0.8614 train_f1: 0.8362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: val_loss: 0.2042 val_precision: 0.9003 val_recall: 0.9356 val_f1: 0.9138\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [04:08<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.1723 train_precision: 0.9364 train_recall: 0.9322 train_f1: 0.9282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: val_loss: 0.2358 val_precision: 0.9245 val_recall: 0.8968 val_f1: 0.9018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [04:08<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.0714 train_precision: 0.9768 train_recall: 0.9813 train_f1: 0.9774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: val_loss: 0.2174 val_precision: 0.9181 val_recall: 0.9480 val_f1: 0.9272\n",
      "Saving model..\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "options = {}\n",
    "options[\"batch_size\"] = BATCH_SIZE\n",
    "options[\"device\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "options[\"train_data\"] = train_data\n",
    "options[\"val_data\"] = val_data\n",
    "options[\"save_path\"] = SAVE_PATH + \"_all_training\"\n",
    "options[\"epochs\"] = EPOCHS\n",
    "options[\"training_type\"] = \"all_training\"\n",
    "trainer = Trainer(options)\n",
    "trainer.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "    def __init__(self, options):\n",
    "        self.save_path = options[\"save_path\"]\n",
    "        self.device = options[\"device\"]\n",
    "        self.test_data = options[\"test_data\"]\n",
    "        self.batch_size = options[\"batch_size\"]\n",
    "        transformer = DistillBERT(self.save_path)\n",
    "        self.model, self.tokenizer = transformer.get_tokenizer_and_model()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def get_performance_metrics(self, preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        precision = precision_score(labels_flat, pred_flat, zero_division=0)\n",
    "        recall = recall_score(labels_flat, pred_flat, zero_division=0)\n",
    "        f1 = f1_score(labels_flat, pred_flat, zero_division=0)\n",
    "        return precision, recall, f1\n",
    "\n",
    "    def test(self, data_loader):\n",
    "        self.model.eval()\n",
    "        total_recall = 0\n",
    "        total_precision = 0\n",
    "        total_f1 = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for reviews, labels in tqdm(data_loader):\n",
    "                reviews = reviews.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                output = self.model(reviews)\n",
    "                logits = output.logits\n",
    "                loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Calculate metrics\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                labels = labels.to(\"cpu\").numpy()\n",
    "                precision, recall, f1 = self.get_performance_metrics(logits, labels)\n",
    "\n",
    "                total_precision += precision\n",
    "                total_recall += recall\n",
    "                total_f1 += f1\n",
    "\n",
    "        precision = total_precision / len(data_loader)\n",
    "        recall = total_recall / len(data_loader)\n",
    "        f1 = total_f1 / len(data_loader)\n",
    "        loss = total_loss / len(data_loader)\n",
    "\n",
    "        return precision, recall, f1, loss\n",
    "\n",
    "    def execute(self):\n",
    "        test_dataset = DatasetLoader(self.test_data, self.tokenizer)\n",
    "        test_data_loader = test_dataset.get_data_loaders(self.batch_size)\n",
    "\n",
    "        test_precision, test_recall, test_f1, test_loss = self.test(test_data_loader)\n",
    "\n",
    "        print()\n",
    "        print(\n",
    "            f\"test_loss: {test_loss:.4f} test_precision: {test_precision:.4f} test_recall: {test_recall:.4f} test_f1: {test_f1:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:00<00:00, 856.25it/s]\n",
      "100%|██████████| 38/38 [00:09<00:00,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test_loss: 0.5830 test_precision: 0.7100 test_recall: 0.7734 test_f1: 0.7290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "options = {}\n",
    "options[\"batch_size\"] = BATCH_SIZE\n",
    "options[\"device\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "options[\"test_data\"] = test_data\n",
    "options[\"save_path\"] = SAVE_PATH + \"_frozen_embeddings\"\n",
    "tester = Tester(options)\n",
    "tester.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 2 Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:00<00:00, 851.76it/s]\n",
      "100%|██████████| 38/38 [00:09<00:00,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test_loss: 0.5865 test_precision: 0.6777 test_recall: 0.8467 test_f1: 0.7405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "options = {}\n",
    "options[\"batch_size\"] = BATCH_SIZE\n",
    "options[\"device\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "options[\"test_data\"] = test_data\n",
    "options[\"save_path\"] = SAVE_PATH + \"_top_2_training\"\n",
    "tester = Tester(options)\n",
    "tester.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 4 Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:00<00:00, 697.47it/s]\n",
      "100%|██████████| 38/38 [00:09<00:00,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test_loss: 0.6227 test_precision: 0.6865 test_recall: 0.7750 test_f1: 0.7139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "options = {}\n",
    "options[\"batch_size\"] = BATCH_SIZE\n",
    "options[\"device\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "options[\"test_data\"] = test_data\n",
    "options[\"save_path\"] = SAVE_PATH + \"_top_4_training\"\n",
    "tester = Tester(options)\n",
    "tester.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:01<00:00, 500.96it/s]\n",
      "100%|██████████| 38/38 [00:09<00:00,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test_loss: 0.3341 test_precision: 0.8590 test_recall: 0.8894 test_f1: 0.8662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "options = {}\n",
    "options[\"batch_size\"] = BATCH_SIZE\n",
    "options[\"device\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "options[\"test_data\"] = test_data\n",
    "options[\"save_path\"] = SAVE_PATH + \"_all_training\"\n",
    "tester = Tester(options)\n",
    "tester.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model             | Phase      | Precision | Recall | F1 Score |\n",
    "| ----------------- | ---------- | --------- | ------ | -------- |\n",
    "| Frozen Embeddings | Testing    | 0.7100    | 0.7734 | 0.7290   |\n",
    "| Top 2 Training    | Testing    | 0.6777    | 0.8467 | 0.7405   |\n",
    "| Top 4 Training    | Testing    | 0.6865    | 0.7750 | 0.7139   |\n",
    "| All Training      | Testing    | 0.8590    | 0.8894 | 0.8662   |\n",
    "| Frozen Embeddings | Validation | 0.7337    | 0.7996 | 0.7524   |\n",
    "| Top 2 Training    | Validation | 0.7117    | 0.8149 | 0.7556   |\n",
    "| Top 4 Training    | Validation | 0.7543    | 0.6898 | 0.7105   |\n",
    "| All Training      | Validation | 0.9181    | 0.9480 | 0.9272   |\n",
    "\n",
    "Freezing embeddings can speed up training and reduce overfitting since the weights of frozen layers are prevented from being updated during the training process. This can be useful when the goal is to preserve the pre-trained knowledge in those layers, especially when working with smaller datasets, but it hinders the model's ability to generalize to new data. As the results show, the Top 2 Training model has a higher F1 score than the Top 4 Training model, due to the latter having a lower recall. Even so, it doesn't have a higher precision and it's important to note that the difference in performance between the two models is relatively small. The performance of models could be affected by factors like random weight initialization, number of epochs, and batch size. Training all layers allows the model to adapt the best; however, it may require more computational resources and have an increased risk of overfitting, especially with smaller datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "This code was written with the help of Dhruv Verma.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
