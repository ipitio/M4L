# analogy.ipynb

In this notebook we:

- Generate batches for the Skip-Gram model,
- Implement NLL and NEG loss functions to train word embeddings,
- Tune the parameters for word embeddings,
- Apply the best learned word embeddings to a word analogy task,
- Calculate bias scores on the best models, and
- Run WEAT tests.
