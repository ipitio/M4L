{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec4Analogies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we:\n",
    "\n",
    "- generate batch for skip-gram model\n",
    "- implement two loss functions to train word embeddings\n",
    "- tune the parameters for word embeddings\n",
    "- apply best learned word embeddings to word analogy task\n",
    "- calculate bias score on your best models\n",
    "- create a new task on which to run WEAT test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data and needed libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datafile\n",
    "!wget http://mattmahoney.net/dc/text8.zip\n",
    "!unzip text8.zip\n",
    "!rm text8.zip\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# seeds for repeatable experiments\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "hw = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from importlib import util\n",
    "if util.find_spec(\"torch_directml\"):\n",
    "    # directml support\n",
    "    import torch_directml\n",
    "    hw = torch_directml.device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train word vectors, we generate training instances from the given data in batches. For the skip-gram model, we slide a window and sample training instances from the data inside the window.\n",
    "\n",
    "<b>For example:</b>\n",
    "\n",
    "Suppose that we have a text: \"The quick brown fox jumps over the lazy dog.\"\n",
    "and batch_size = 8, window_size = 3\n",
    "\n",
    "\"<font color = red>[The quick brown]</font> fox jumps over the lazy dog\"\n",
    "\n",
    "Context word would be 'quick' and predicting words are 'The' and 'brown'.\n",
    "\n",
    "This will generate training examples of the form context(x), predicted_word(y) like:\n",
    "\n",
    "<ul>\n",
    "      <li>(quick    ,       The)\n",
    "      <li>(quick    ,     brown)\n",
    "</ul>\n",
    "And then move the sliding window.\n",
    "\n",
    "\"The <font color = red>[quick brown fox]</font> jumps over the lazy dog\"\n",
    "\n",
    "In the same way, we have two more examples:\n",
    "\n",
    "<ul>\n",
    "    <li>(brown, quick)\n",
    "    <li>(brown, fox)\n",
    "</ul>\n",
    "\n",
    "Moving the window again:\n",
    "\n",
    "\"The quick <font color = red>[brown fox jumps]</font> over the lazy dog\"\n",
    "\n",
    "We get,\n",
    "\n",
    "<ul>\n",
    "    <li>(fox, brown)\n",
    "    <li>(fox, jumps)\n",
    "</ul>\n",
    "\n",
    "Finally we get two more instances from the moved window,\n",
    "\n",
    "\"The quick brown <font color = red>[fox jumps over]</font> the lazy dog\"\n",
    "\n",
    "<ul>\n",
    "    <li>(jumps, fox)\n",
    "    <li>(jumps, over)\n",
    "</ul>\n",
    "\n",
    "Since now we have 8 training instances, which is the batch size, we will stop generating this batch and return batch data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    with open(filename) as file:\n",
    "        text = file.read()\n",
    "        data = [token.lower() for token in text.strip().split(\" \")]\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_dataset(words, vocab_size):\n",
    "    count = [[\"UNK\", -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocab_size - 1))\n",
    "    # token_to_id dictionary, id_to_taken reverse_dictionary\n",
    "    vocab_token_to_id = dict()\n",
    "    for word, _ in count:\n",
    "        vocab_token_to_id[word] = len(vocab_token_to_id)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in vocab_token_to_id:\n",
    "            index = vocab_token_to_id[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    vocab_id_to_token = dict(zip(vocab_token_to_id.values(), vocab_token_to_id.keys()))\n",
    "    return data, count, vocab_token_to_id, vocab_id_to_token\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, data, batch_size=128, num_skips=8, skip_window=4):\n",
    "        \"\"\"\n",
    "        @data_index: the index of a word. You can access a word using data[data_index]\n",
    "        @batch_size: the number of instances in one batch\n",
    "        @num_skips: the number of samples you want to draw in a window\n",
    "                (In the below example, it was 2)\n",
    "        @skip_window: decides how many words to consider left and right from a context word.\n",
    "                    (So, skip_windows*2+1 = window_size)\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_index = 0\n",
    "        self.data = data\n",
    "        assert batch_size % num_skips == 0\n",
    "        assert num_skips <= 2 * skip_window\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_skips = num_skips\n",
    "        self.skip_window = skip_window\n",
    "\n",
    "    def reset_index(self, idx=0):\n",
    "        self.data_index = idx\n",
    "\n",
    "    def generate_batch(self):\n",
    "        \"\"\"\n",
    "        Collect the center and context words for each batch. Then return them as a tuple of tensors.\n",
    "\n",
    "        batch will contain word ids for context words. Dimension is [batch_size].\n",
    "        labels will contain word ids for predicting(target) words. Dimension is [batch_size, 1].\n",
    "        \"\"\"\n",
    "\n",
    "        center_word = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "        context_word = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "\n",
    "        # stride: for the rolling window\n",
    "        stride = 1\n",
    "\n",
    "        # for each batch\n",
    "        for i in range(self.batch_size):\n",
    "            # get the center and context words\n",
    "            center_word[i] = self.data[self.data_index]\n",
    "            context_word[i] = self.data[self.data_index + stride]\n",
    "            # update the stride and data_index\n",
    "            stride += 1\n",
    "            if stride > self.skip_window:\n",
    "                stride = 1\n",
    "            if self.data_index >= len(self.data):\n",
    "                self.data_index = 0\n",
    "            self.data_index += 1\n",
    "\n",
    "        return torch.LongTensor(center_word).to(hw), torch.LongTensor(context_word).to(\n",
    "            hw\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Negative Log Likelihood (NLL): </b>\n",
    "This is the negative of the log likelihood function, which is a loss function since it measures how bad the current model is from the expected behavior. Please refer to Stanford's CS224n [Lecture Notes](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf) for more details.\n",
    "\n",
    "<b>Negative Sampling (NEG): </b>\n",
    "The negative sampling formulates a slightly different classification task and corresponding loss. [This paper](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) describes the method in detail. The idea here is to build a classifier that can give high probabilities to words that are the correct target words and low probabilities to words that are incorrect target words. As with negative log likelihood loss, we define the classifier using a function that uses the word vectors of the context and target as free parameters. The key difference is that instead of using the entire vocabulary, we sample a set of k negative words for each instance and create an augmented instance, which is a collection of the true target word and k negative words. Now the vectors are trained to maximize the probability of this augmented instance. You may again refer to the Lecture Notes linked above for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip x to avoid overflow and underflow problems\n",
    "sigmoid = lambda x: 1 / (1 + torch.exp(-x.clamp(-10, 10)))\n",
    "\n",
    "\n",
    "class WordVec(nn.Module):\n",
    "    def __init__(\n",
    "        self, V, embedding_dim, loss_func, counts, num_neg_samples_per_center=1\n",
    "    ):\n",
    "        super(WordVec, self).__init__()\n",
    "        self.center_embeddings = nn.Embedding(\n",
    "            num_embeddings=V, embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.center_embeddings.weight.data.normal_(\n",
    "            mean=0, std=1 / math.sqrt(embedding_dim)\n",
    "        )\n",
    "        self.center_embeddings.weight.data[self.center_embeddings.weight.data < -1] = -1\n",
    "        self.center_embeddings.weight.data[self.center_embeddings.weight.data > 1] = 1\n",
    "\n",
    "        self.context_embeddings = nn.Embedding(\n",
    "            num_embeddings=V, embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.context_embeddings.weight.data.normal_(\n",
    "            mean=0, std=1 / math.sqrt(embedding_dim)\n",
    "        )\n",
    "        self.context_embeddings.weight.data[\n",
    "            self.context_embeddings.weight.data < -1\n",
    "        ] = (-1 + 1e-10)\n",
    "        self.context_embeddings.weight.data[self.context_embeddings.weight.data > 1] = (\n",
    "            1 - 1e-10\n",
    "        )\n",
    "\n",
    "        self.loss_func = loss_func\n",
    "        self.counts = counts\n",
    "\n",
    "        self.num_neg_samples_per_center = num_neg_samples_per_center\n",
    "\n",
    "    def forward(self, center_word, context_word):\n",
    "        if self.loss_func == \"nll\":\n",
    "            return self.negative_log_likelihood_loss(center_word, context_word)\n",
    "        elif self.loss_func == \"neg\":\n",
    "            return self.negative_sampling(center_word, context_word)\n",
    "        else:\n",
    "            raise Exception(\"No implementation found for %s\" % (self.loss_func))\n",
    "\n",
    "    def negative_log_likelihood_loss(self, center_word, context_word):\n",
    "        center_word_embeddings = self.center_embeddings(center_word)  # batches, dims\n",
    "        context_word_embeddings = self.context_embeddings(context_word)  # batches, dims\n",
    "\n",
    "        a = torch.sum(\n",
    "            torch.mul(center_word_embeddings, context_word_embeddings), dim=1\n",
    "        )  # batches\n",
    "        # clamp @ to avoid overflow and underflow problems\n",
    "        b = torch.log(\n",
    "            torch.sum(\n",
    "                torch.exp(\n",
    "                    torch.clamp(\n",
    "                        torch.mm(\n",
    "                            center_word_embeddings, self.context_embeddings.weight.t()\n",
    "                        ),\n",
    "                        -10,\n",
    "                        10,\n",
    "                    )\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "        )\n",
    "        loss = torch.mean(torch.log(1 + torch.exp(b - a)))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def negative_sampling(self, center_word, context_word):\n",
    "        # control the number of negative samples for every positive sample\n",
    "        num_neg_samples_per_center = self.num_neg_samples_per_center\n",
    "\n",
    "        center_word_embeddings = self.center_embeddings(center_word)\n",
    "        context_word_embeddings = self.context_embeddings(context_word)\n",
    "\n",
    "        # get the number of batches and dimensions\n",
    "        num_batches = center_word_embeddings.shape[0]\n",
    "        num_dims = center_word_embeddings.shape[1]\n",
    "        num_neg_samples = num_batches * num_neg_samples_per_center\n",
    "\n",
    "        # sample negative samples\n",
    "        neg_samples = torch.multinomial(\n",
    "            torch.Tensor(self.counts).to(hw),\n",
    "            num_samples=num_neg_samples,\n",
    "            replacement=True,\n",
    "        )\n",
    "        neg_sample_embeddings = self.context_embeddings(neg_samples)\n",
    "\n",
    "        # reshape embeddings\n",
    "        center_word_embeddings = center_word_embeddings.view(num_batches, 1, num_dims)\n",
    "        context_word_embeddings = context_word_embeddings.view(num_batches, 1, num_dims)\n",
    "        neg_sample_embeddings = neg_sample_embeddings.view(\n",
    "            num_batches, num_neg_samples_per_center, num_dims\n",
    "        )\n",
    "\n",
    "        # dot product\n",
    "        center_context_dot_product = torch.sum(\n",
    "            torch.mul(center_word_embeddings, context_word_embeddings), dim=2\n",
    "        )\n",
    "        center_neg_sample_dot_product = torch.sum(\n",
    "            torch.mul(center_word_embeddings, neg_sample_embeddings), dim=2\n",
    "        )\n",
    "\n",
    "        # log sigmoid\n",
    "        log_sigmoid_center_context = torch.log(sigmoid(center_context_dot_product))\n",
    "        log_sigmoid_center_neg_sample = torch.log(\n",
    "            sigmoid(-center_neg_sample_dot_product)\n",
    "        )\n",
    "\n",
    "        # loss\n",
    "        loss = -torch.mean(\n",
    "            log_sigmoid_center_context + torch.sum(log_sigmoid_center_neg_sample, dim=1)\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def print_closest(self, validation_words, reverse_dictionary, top_k=8):\n",
    "        print(\"Printing closest words\")\n",
    "        embeddings = torch.zeros(self.center_embeddings.weight.shape).copy_(\n",
    "            self.center_embeddings.weight\n",
    "        )\n",
    "        embeddings = embeddings.data.cpu().numpy()\n",
    "\n",
    "        validation_ids = validation_words\n",
    "        norm = np.sqrt(np.sum(np.square(embeddings), axis=1, keepdims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        validation_embeddings = normalized_embeddings[validation_ids]\n",
    "        similarity = np.matmul(validation_embeddings, normalized_embeddings.T)\n",
    "        for i in range(len(validation_ids)):\n",
    "            word = reverse_dictionary[validation_words[i]]\n",
    "            nearest = (-similarity[i, :]).argsort()[1 : top_k + 1]\n",
    "            print(word, [reverse_dictionary[nearest[k]] for k in range(top_k)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Data Loading Loops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses the models and losses built above and runs the actual training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, ckpt_save_path, reverse_dictionary):\n",
    "        self.model = model\n",
    "        self.ckpt_save_path = ckpt_save_path\n",
    "        self.reverse_dictionary = reverse_dictionary\n",
    "\n",
    "    def training_step(self, center_word, context_word):\n",
    "        loss = self.model(center_word, context_word)\n",
    "        return loss\n",
    "\n",
    "    def train(\n",
    "        self, dataset, max_training_steps, ckpt_steps, validation_words, device=hw, lr=1\n",
    "    ):\n",
    "        optim = torch.optim.SGD(self.model.parameters(), lr=lr, weight_decay=1e-6)\n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "        self.losses = []\n",
    "\n",
    "        # trace errors\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "        # register hook to clip gradients during backprop\n",
    "        for p in self.model.parameters():\n",
    "            p.register_hook(lambda grad: torch.clamp(grad, -1e6, 1e6))\n",
    "\n",
    "        t = tqdm(range(max_training_steps))\n",
    "        for curr_step in t:\n",
    "            optim.zero_grad()\n",
    "            center_word, context_word = dataset.generate_batch()\n",
    "            loss = self.training_step(center_word.to(device), context_word.to(device))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            self.losses.append(loss.item())\n",
    "            if curr_step:\n",
    "                t.set_description(\n",
    "                    \"Avg loss: %s\"\n",
    "                    % (round(sum(self.losses[-2000:]) / len(self.losses[-2000:]), 3))\n",
    "                )\n",
    "            if curr_step % 10000 == 0:\n",
    "                self.model.print_closest(validation_words, self.reverse_dictionary)\n",
    "            if curr_step % ckpt_steps == 0 and curr_step > 0:\n",
    "                self.save_ckpt(curr_step)\n",
    "\n",
    "    def save_ckpt(self, curr_step):\n",
    "        torch.save(self.model, \"%s/%s.pt\" % (self.ckpt_save_path, str(curr_step)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following run_training function will train a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "        print(\"Created a path: %s\" % (path))\n",
    "\n",
    "\n",
    "def run_training(\n",
    "    model_type=\"nll\",  # loss function: 'nll' or 'neg'\n",
    "    lr=1.0,\n",
    "    num_neg_samples_per_center=1,  # negative samples per center word\n",
    "    checkpoint_model_path=\"./checkpoints\",\n",
    "    final_model_path=\"./final_model\",\n",
    "    skip_window=1,  # skip window size\n",
    "    vocab_size=int(1e5),\n",
    "    num_skips=2,  # num samples to be drawn from a window\n",
    "    batch_size=64,  # number of x,y pairs in a batch\n",
    "    embedding_size=128,  # embedding vector size\n",
    "    checkpoint_step=50000,  # steps between saving checkpoints\n",
    "    max_num_steps=200001,  # max number of steps to train\n",
    "):\n",
    "    # if checkpoint path exists, continue training from the checkpoint\n",
    "    point = 0\n",
    "    if os.path.exists(checkpoint_model_path):\n",
    "        point = 1\n",
    "    checkpoint_model_path = f\"{checkpoint_model_path}_{model_type}/\"\n",
    "    create_path(checkpoint_model_path)\n",
    "\n",
    "    # Read data\n",
    "    words = read_data(\"./text8\")\n",
    "    data, count, vocab_token_to_id, vocab_id_to_token = build_dataset(words, vocab_size)\n",
    "\n",
    "    print(\"Data size\", len(words))\n",
    "    print(\"Most common words (+UNK)\", count[:5])\n",
    "    print(\"Sample data\", data[:10], [vocab_id_to_token[i] for i in data[:10]])\n",
    "\n",
    "    # Calculate the probability of unigrams\n",
    "    # unigram_cnt = [c for w, c in count]\n",
    "    count_dict = dict(count)\n",
    "    unigram_cnt = [\n",
    "        count_dict[vocab_id_to_token[i]]\n",
    "        for i in sorted(list(vocab_token_to_id.values()))\n",
    "    ]\n",
    "    unigram_cnt = np.array(unigram_cnt) ** (3 / 4)\n",
    "    unigram_cnt = unigram_cnt / np.sum(unigram_cnt)\n",
    "\n",
    "    dataset = Dataset(\n",
    "        data, batch_size=batch_size, num_skips=num_skips, skip_window=skip_window\n",
    "    )\n",
    "    center, context = dataset.generate_batch()\n",
    "    for i in range(8):\n",
    "        print(\n",
    "            center[i].item(),\n",
    "            vocab_id_to_token[center[i].item()],\n",
    "            \"->\",\n",
    "            context[i].item(),\n",
    "            vocab_id_to_token[context[i].item()],\n",
    "        )\n",
    "    dataset.reset_index()\n",
    "\n",
    "    valid_size = 16  # random set of words to evaluate similarity on\n",
    "    valid_window = 100  # only pick dev samples in the head of the distribution\n",
    "    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "    embedding_size = embedding_size\n",
    "    model = WordVec(\n",
    "        V=vocab_size,\n",
    "        embedding_dim=embedding_size,\n",
    "        loss_func=model_type,\n",
    "        counts=np.array(unigram_cnt),\n",
    "        num_neg_samples_per_center=num_neg_samples_per_center,\n",
    "    )\n",
    "    if point:\n",
    "        model = torch.load(checkpoint_model_path)\n",
    "        print(\"Loaded model from checkpoint\")\n",
    "    trainer = Trainer(model, checkpoint_model_path, vocab_id_to_token)\n",
    "    device = hw\n",
    "    print(f\"Device: {device}\")\n",
    "    trainer.train(\n",
    "        dataset, max_num_steps, checkpoint_step, valid_examples, device, lr=lr\n",
    "    )\n",
    "    model_path = final_model_path\n",
    "    create_path(model_path)\n",
    "    model_filepath = os.path.join(model_path, \"word2vec_%s.model\" % (model_type))\n",
    "    pickle.dump(\n",
    "        [vocab_token_to_id, model.center_embeddings.weight.detach().cpu().numpy()],\n",
    "        open(model_filepath, \"wb\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When choosing hyperparamters, one must be careful. If the learning rate it is too big, the function might not be able to reach the minimum of the gradient descent. If it is too small, it will take a long time to reach the minimum. Likewise, if the window size or number of negative samples are too big, the model will take a long time to train. If they are too small, the model will not be able to learn the context of the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(\n",
    "    model_type=\"neg\",\n",
    "    checkpoint_model_path=\"./neg_checkpoints\",\n",
    "    final_model_path=\"./final_neg_model\",\n",
    "    num_neg_samples_per_center=5,\n",
    "    lr=6.4,\n",
    "    skip_window=7,\n",
    ")\n",
    "\n",
    "run_training(\n",
    "    model_type=\"nll\",\n",
    "    checkpoint_model_path=\"./nll_checkpoints\",\n",
    "    final_model_path=\"./final_nll_model\",\n",
    "    num_neg_samples_per_center=5,\n",
    "    lr=6.4,\n",
    "    skip_window=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing: Analogies with Word Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word vectors can now be used for word analogy tasks. Each one has example pairs of a certain relation; the most/least illustrative word pair of the relation is to be found. One simple method is measuring the similarities of difference vectors: if (a, b) and (c, d) pairs are analogous pairs then the transformation from a to b (i.e., some x vector when added to a gives b: a + x = b) should be highly similar to the transformation from c to d (i.e., some y vector when added to c gives d: c + y = d). In other words, the difference vector (b-a) should be similar to difference vector (d-c). This difference vector can be thought to represent the relation between the two words.\n",
    "\n",
    "Due to the noisy annotation data, the expected accuracy is not high.\n",
    "\n",
    "Each task is in the following form:\n",
    "\n",
    "    Consider the following word pairs that share the same relation, R:\n",
    "\n",
    "    pilgrim:shrine, hunter:quarry, assassin:victim, climber:peak\n",
    "\n",
    "    Among these word pairs,\n",
    "\n",
    "    (1) pig:mud\n",
    "    (2) politician:votes\n",
    "    (3) dog:bone\n",
    "    (4) bird:worm\n",
    "\n",
    "    Q1. Which word pairs has the MOST illustrative(similar) example of the relation R?\n",
    "    Q2. Which word pairs has the LEAST illustrative(similar) example of the relation R?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_split(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    candidate, test = [], []\n",
    "    for line in data:\n",
    "        a, b = line.strip().split(\"||\")\n",
    "        a = [i[1:-1].split(\":\") for i in a.split(\",\")]\n",
    "        b = [i[1:-1].split(\":\") for i in b.split(\",\")]\n",
    "        candidate.append(a)\n",
    "        test.append(b)\n",
    "\n",
    "    return candidate, test\n",
    "\n",
    "\n",
    "def get_embeddings(examples, embeddings, dictionary):\n",
    "    \"\"\"\n",
    "    For the word pairs in the 'examples' array, fetch embeddings and return.\n",
    "    \"\"\"\n",
    "\n",
    "    norm = np.sqrt(np.sum(np.square(embeddings), axis=1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "\n",
    "    embs = []\n",
    "    for line in examples:\n",
    "        temp = []\n",
    "        for pairs in line:\n",
    "            temp.append(\n",
    "                [\n",
    "                    normalized_embeddings[dictionary[pairs[0]]],\n",
    "                    normalized_embeddings[dictionary[pairs[1]]],\n",
    "                ]\n",
    "            )\n",
    "        embs.append(temp)\n",
    "\n",
    "    result = embs\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate_pairs(candidate_embs, test_embs):\n",
    "    \"\"\"\n",
    "    Find the best and worst pairs and return that.\n",
    "    \"\"\"\n",
    "\n",
    "    best_pairs = []\n",
    "    worst_pairs = []\n",
    "\n",
    "    # calculate the norm between the candidate and test embs\n",
    "    for i, line in enumerate(test_embs):\n",
    "        norms = []\n",
    "        for j, pairs in enumerate(candidate_embs[i]):\n",
    "            norms.append(\n",
    "                np.linalg.norm(line[0] - pairs[0]) + np.linalg.norm(line[1] - pairs[1])\n",
    "            )\n",
    "        best_pairs.append(np.argmin(norms))\n",
    "        worst_pairs.append(np.argmax(norms))\n",
    "\n",
    "    return best_pairs, worst_pairs\n",
    "\n",
    "\n",
    "def write_solution(best_pairs, worst_pairs, test, path):\n",
    "    \"\"\"\n",
    "    Write best and worst pairs to a file, that can be evaluated by score_maxdiff.pl\n",
    "    \"\"\"\n",
    "\n",
    "    ans = []\n",
    "    for i, line in enumerate(test):\n",
    "        temp = [f'\"{pairs[0]}:{pairs[1]}\"' for pairs in line]\n",
    "        # handle the case where the best or worst pair is not in the candidate set\n",
    "        try:\n",
    "            temp.append(f'\"{line[worst_pairs[i]][0]}:{line[worst_pairs[i]][1]}\"')\n",
    "            temp.append(f'\"{line[best_pairs[i]][0]}:{line[best_pairs[i]][1]}\"')\n",
    "            ans.append(\" \".join(temp))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(ans))\n",
    "\n",
    "\n",
    "def run_word_analogy_eval(\n",
    "    model_path=\"./final_model\",  # path to the model\n",
    "    input_filepath=\"word_analogy_dev.txt\",  # word analogy file to evaluate on\n",
    "    output_filepath=\"word_analogy_results.txt\",  # predicted results\n",
    "    model_type=\"nll\",  # loss: NLL or NEG\n",
    "):\n",
    "    print(f\"Model file: {model_path}/word2vec_{model_type}.model\")\n",
    "    model_filepath = os.path.join(model_path, \"word2vec_%s.model\" % (model_type))\n",
    "\n",
    "    dictionary, embeddings = pickle.load(open(model_filepath, \"rb\"))\n",
    "\n",
    "    candidate, test = read_data_split(input_filepath)\n",
    "\n",
    "    candidate_embs = get_embeddings(candidate, embeddings, dictionary)\n",
    "    test_embs = get_embeddings(test, embeddings, dictionary)\n",
    "\n",
    "    best_pairs, worst_pairs = evaluate_pairs(candidate_embs, test_embs)\n",
    "\n",
    "    out_filepath = output_filepath\n",
    "    print(f\"Output file: {out_filepath}\")\n",
    "    write_solution(best_pairs, worst_pairs, test, out_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the results of the word analogy task and convert into numeric metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_word_analogy_eval(\n",
    "    model_path=\"./final_neg_model\",\n",
    "    input_filepath=\"word_analogy_dev.txt\",\n",
    "    output_filepath=\"word_analogy_dev_results_neg.txt\",\n",
    "    model_type=\"neg\",\n",
    ")\n",
    "\n",
    "run_word_analogy_eval(\n",
    "    model_path=\"./final_neg_model\",\n",
    "    input_filepath=\"word_analogy_test.txt\",\n",
    "    output_filepath=\"word_analogy_test_results_neg.txt\",\n",
    "    model_type=\"neg\",\n",
    ")\n",
    "\n",
    "run_word_analogy_eval(\n",
    "    model_path=\"./final_nll_model\",\n",
    "    input_filepath=\"word_analogy_dev.txt\",\n",
    "    output_filepath=\"word_analogy_dev_results_nll.txt\",\n",
    "    model_type=\"nll\",\n",
    ")\n",
    "\n",
    "run_word_analogy_eval(\n",
    "    model_path=\"./final_nll_model\",\n",
    "    input_filepath=\"word_analogy_test.txt\",\n",
    "    output_filepath=\"word_analogy_test_results_nll.txt\",\n",
    "    model_type=\"nll\",\n",
    ")\n",
    "\n",
    "!chmod 777 score_maxdiff.pl\n",
    "!./score_maxdiff.pl word_analogy_dev_mturk_answers.txt word_analogy_dev_results_neg.txt score_neg.txt\n",
    "!./score_maxdiff.pl word_analogy_dev_mturk_answers.txt word_analogy_dev_results_nll.txt score_nll.txt\n",
    "!./score_maxdiff.pl word_analogy_dev_mturk_answers.txt word_analogy_test_results_neg.txt score_neg.txt\n",
    "!./score_maxdiff.pl word_analogy_dev_mturk_answers.txt word_analogy_test_results_nll.txt score_nll.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEAT Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While word embeddings can help us learn analogies, they can also be biased. The WEAT test provides a way to empirically measure said bias. [This paper](https://arxiv.org/pdf/1810.03611.pdf) describes the method in detail. It measures the degree to which a model associates sets of target words (e.g., African American names, European American names, flowers, insects) with sets of attribute words (e.g., ”stable”, ”pleasant” or ”unpleasant”). The association between two given words is defined as the cosine similarity between the embedding vectors for the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n",
    "        return True\n",
    "    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n",
    "        return False\n",
    "\n",
    "\n",
    "def unit_vector(vec):\n",
    "    return vec / np.linalg.norm(vec)\n",
    "\n",
    "\n",
    "def cos_sim(v1, v2):\n",
    "    \"\"\"\n",
    "    Cosine Similarity between the 2 vectors\n",
    "    \"\"\"\n",
    "\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.clip(np.tensordot(v1_u, v2_u, axes=(-1, -1)), -1.0, 1.0)\n",
    "\n",
    "\n",
    "def weat_association(W, A, B):\n",
    "    \"\"\"\n",
    "    Compute Weat score for given target words W, along the attributes A & B.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.mean(cos_sim(W, A), axis=-1) - np.mean(cos_sim(W, B), axis=-1)\n",
    "\n",
    "\n",
    "def weat_score(X, Y, A, B):\n",
    "    \"\"\"\n",
    "    Compute differential weat score across the given target words X & Y along the attributes A & B.\n",
    "    \"\"\"\n",
    "\n",
    "    x_association = weat_association(X, A, B)\n",
    "    y_association = weat_association(Y, A, B)\n",
    "\n",
    "    tmp1 = np.mean(x_association, axis=-1) - np.mean(y_association, axis=-1)\n",
    "    tmp2 = np.std(np.concatenate((x_association, y_association), axis=0))\n",
    "\n",
    "    return tmp1 / tmp2\n",
    "\n",
    "\n",
    "def balance_word_vectors(vec1, vec2):\n",
    "    diff = len(vec1) - len(vec2)\n",
    "\n",
    "    if diff > 0:\n",
    "        vec1 = np.delete(vec1, np.random.choice(len(vec1), diff, 0), axis=0)\n",
    "    else:\n",
    "        vec2 = np.delete(vec2, np.random.choice(len(vec2), -diff, 0), axis=0)\n",
    "\n",
    "    return (vec1, vec2)\n",
    "\n",
    "\n",
    "def get_word_vectors(words, model, vocab_token_to_id):\n",
    "    \"\"\"\n",
    "    Return list of word embeddings for the given words using the passed model and tokeniser\n",
    "    \"\"\"\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            output.append(model[vocab_token_to_id[word]])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return np.array(output)\n",
    "\n",
    "\n",
    "def compute_weat(weat_path, model, vocab_token_to_id):\n",
    "    \"\"\"\n",
    "    Compute WEAT score for the task as defined in the file at `weat_path`, and generating word embeddings from the passed model and tokeniser.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(weat_path) as f:\n",
    "        weat_dict = json.load(f)\n",
    "\n",
    "    all_scores = {}\n",
    "\n",
    "    for data_name, data_dict in weat_dict.items():\n",
    "        # Target\n",
    "        X_key = data_dict[\"X_key\"]\n",
    "        Y_key = data_dict[\"Y_key\"]\n",
    "\n",
    "        # Attributes\n",
    "        A_key = data_dict[\"A_key\"]\n",
    "        B_key = data_dict[\"B_key\"]\n",
    "\n",
    "        X = get_word_vectors(data_dict[X_key], model, vocab_token_to_id)\n",
    "        Y = get_word_vectors(data_dict[Y_key], model, vocab_token_to_id)\n",
    "        A = get_word_vectors(data_dict[A_key], model, vocab_token_to_id)\n",
    "        B = get_word_vectors(data_dict[B_key], model, vocab_token_to_id)\n",
    "\n",
    "        if len(X) == 0 or len(Y) == 0:\n",
    "            print(\"Not enough matching words in dictionary\")\n",
    "            continue\n",
    "\n",
    "        X, Y = balance_word_vectors(X, Y)\n",
    "        A, B = balance_word_vectors(A, B)\n",
    "\n",
    "        score = weat_score(X, Y, A, B)\n",
    "        all_scores[data_name] = str(score)\n",
    "\n",
    "    return all_scores\n",
    "\n",
    "\n",
    "def dump_dict(obj, output_path):\n",
    "    with open(output_path, \"w\") as file:\n",
    "        json.dump(obj, file)\n",
    "\n",
    "\n",
    "def run_bias_eval(\n",
    "    weat_file_path=\"weat.json\",  # weat file where the tasks are defined\n",
    "    out_file=\"weat_results.json\",  # output JSON file where the output is stored\n",
    "    model_path=\"/content/final_model/word2vec_nll.model\",  # Full model path (including filename) to load from\n",
    "):\n",
    "    vocab_token_to_id, model = pickle.load(open(model_path, \"rb\"))\n",
    "\n",
    "    bias_score = compute_weat(weat_file_path, model, vocab_token_to_id)\n",
    "\n",
    "    print(\"Final Bias Scores\")\n",
    "    print(json.dumps(bias_score, indent=4))\n",
    "\n",
    "    dump_dict(bias_score, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the models were trained and tested on the same data and not debiased, bias is expected. Further, the bias is expected to be in favor of the majority group and to follow societal stereotypes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_bias_eval(\n",
    "    weat_file_path=\"weat.json\",\n",
    "    out_file=\"neg_bias_output.json\",\n",
    "    model_path=\"./final_neg_model\",\n",
    ")\n",
    "\n",
    "run_bias_eval(\n",
    "    weat_file_path=\"weat.json\",\n",
    "    out_file=\"nll_bias_output.json\",\n",
    "    model_path=\"./final_nll_model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the WEAT tests, we can see that the NLL model's bias scores are all only slightly negative or positive, but the NEG model's are more extreme. They seem mostly in line with our society's cultural biases; the deviations are likely due to the limited size of the training data. There are many ways to remove bias, such as training on a more diverse set of data, with the help of the debiasing conceptor, data augmentation, and neutralization. However, it is important to note that bias cannot be completely removed, and that the methods used to remove bias may have unintended consequences (like introducing new biases).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was written with the help of Heeyoung Kwon, Jun Kang, Mohaddeseh Bastan, Harsh Trivedi, Matthew Matero, Nikita Soni, Sharvil Katariya, Yash Kumar Lal, Adithya V. Ganesan, Sounak Mondal, Saqib Hasan, Jasdeep Grover, and others. It is not subject to the license of this repository.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "7m0Q604fnADQ"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
